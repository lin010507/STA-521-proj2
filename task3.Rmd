---
title: "task2"
author: "Huiying"
date: "2022-11-27"
output: html_document
---

```{r}
library(MASS)
library(class)
library(randomForest)
library(tidyverse)
library(caret)
library(gbm)
library(e1071)
library(pROC)
```


# a

```{r}
## test accuracy
testacc=function(classifier, train, test){
  classifiers=c("logistic","LDA","QDA","Naive Bayes","knn","rf","adaboost")
  if(!(classifier %in% classifiers)){
    print("Please choose classifiers from logistic, LDA, QDA, Naive Bayes, knn, Random Forest, Adaboost.")
    break
  }
  datatrain = train %>% 
    filter(label != 0)
  datatrain$label[datatrain$label == -1] = 0
  datatest = test %>% 
    filter(label != 0)
  datatest$label[datatest$label == -1] = 0
  
  model_formula = as.formula("label~NDAI+SD+CORR+DF+CF+BF+AF+AN")
  
  # logistic regression
  if(classifier == "logistic"){
    glm.fit = glm(model_formula, data = datatrain, family = binomial)
    glm.probs = predict(glm.fit, datatest, type="response")
    glm.pred = rep(0, length(glm.probs))
    glm.pred[glm.probs > 0.5] = 1
    pred = glm.pred
  }
    
  # LDA
  if(classifier == "LDA"){
    lda.fit = lda(model_formula, data = datatrain)
    lda.pred = predict(lda.fit, datatest)
    pred = lda.pred$class
  }
    
  # QDA
  if(classifier == "QDA"){
    qda.fit = qda(model_formula, data = datatrain)
    qda.pred = predict(qda.fit, datatest)
    pred = qda.pred$class
  }
    
  # Naive Bayes
  if(classifier == "Naive Bayes"){
    nb.fit = naiveBayes(model_formula, data = datatrain)
    nb.pred = predict(nb.fit, datatest)
    pred = nb.pred
  }
    
  # KNN
  if(classifier == "knn"){
    train.X = scale(datatrain[, c("NDAI", "SD", "CORR", "DF", "CF", "BF", "AF", "AN")])
    test.X = scale(datatest[, c("NDAI", "SD", "CORR", "DF", "CF", "BF", "AF", "AN")])
    knn.pred = knn(train.X, test.X, datatrain$label, k=10)
    pred = knn.pred
  }
  
  # Random forest
  if (classifier == "rf"){
      
    train.X = datatrain[, c("NDAI", "SD", "CORR", "DF", "CF", "BF", "AF", "AN")]
    test.X = datatest[, c("NDAI", "SD", "CORR", "DF", "CF", "BF", "AF", "AN")]
    rf = randomForest(x = train.X, y = as.factor(datatrain$label), mtry = 3)
    pred = predict(rf, datatest)
      
  }
    
  # Adaboost
  if (classifier == "adaboost"){
      
    boost = gbm(as.character(label) ~ NDAI+SD+CORR+DF+CF+BF+AF+AN, 
                data = datatrain, 
                distribution = "adaboost", 
                n.trees = 3000, 
                interaction.depth = 4)
      
    pred = predict(boost, datatest)
    pred[pred >= 0.5] = 1
    pred[pred < 0.5] = 0
  }  
  
  acc=mean(pred==datatest$label)
  return(acc)
}
```

```{r}
acc.test=matrix(NA,nrow=7,ncol=2)
colnames(acc.test)=c("systematic","buffering")
rownames(acc.test)=c("logistic","LDA","QDA","Naive Bayes","knn","Random Forest","Adaboost")
acc.test[1,1]=testacc("logistic",image.syst.train,image.syst.test)
acc.test[2,1]=testacc("LDA",image.syst.train,image.syst.test)
acc.test[3,1]=testacc("QDA",image.syst.train,image.syst.test)
acc.test[4,1]=testacc("Naive Bayes",image.syst.train,image.syst.test)
acc.test[5,1]=testacc("knn",image.syst.train,image.syst.test)
acc.test[6,1]=testacc("rf",image.syst.train,image.syst.test)
acc.test[7,1]=testacc("adaboost",image.syst.train,image.syst.test)
acc.test[1,2]=testacc("logistic",image.buf.train,image.buf.test)
acc.test[2,2]=testacc("LDA",image.buf.train,image.buf.test)
acc.test[3,2]=testacc("QDA",image.buf.train,image.buf.test)
acc.test[4,2]=testacc("Naive Bayes",image.buf.train,image.buf.test)
acc.test[5,2]=testacc("knn",image.buf.train,image.buf.test)
acc.test[6,2]=testacc("rf",image.buf.train,image.buf.test)
acc.test[7,2]=testacc("adaboost",image.buf.train,image.buf.test)
acc.test
```

# b

```{r}
## systematic
datatrain = image.syst.train %>% 
  filter(label != 0)
datatrain$label[datatrain$label == -1] = 0
datatest = image.syst.test %>% 
  filter(label != 0)
datatest$label[datatest$label == -1] = 0
  
model_formula = as.formula("label~NDAI+SD+CORR+DF+CF+BF+AF+AN")
  
# logistic regression
glm.fit = glm(model_formula, data = datatrain, family = binomial)
glm.probs = predict(glm.fit, datatest, type="response")
glm.pred = rep(0, length(glm.probs))
glm.pred[glm.probs > 0.5] = 1
glm.roc=roc(datatest$label,glm.probs)
    
# LDA
lda.fit = lda(model_formula, data = datatrain)
lda.pred = predict(lda.fit, datatest)
lda.score=lda.pred$posterior[,2]
lda.roc=roc(datatest$label,lda.score)
    
# QDA
qda.fit = qda(model_formula, data = datatrain)
qda.pred = predict(qda.fit, datatest)
qda.score=qda.pred$posterior[,2]
qda.roc=roc(datatest$label,qda.score)

# Naive Bayes
nb.fit = naiveBayes(model_formula, data = datatrain)
nb.pred = predict(nb.fit, datatest)
nb.score=predict(nb.fit,datatest,type="raw")[,2]
nb.roc=roc(datatest$label,nb.score)
  
# KNN
train.X = scale(datatrain[, c("NDAI", "SD", "CORR", "DF", "CF", "BF", "AF", "AN")])
test.X = scale(datatest[, c("NDAI", "SD", "CORR", "DF", "CF", "BF", "AF", "AN")])
knn.pred = knn(train.X, test.X, datatrain$label, k=10,prob=TRUE)
knn.score = attr(knn.pred,"prob")
for(i in 1:length(knn.score)){
  if(knn.pred[i]==0){
    knn.score[i]=1-knn.score[i]
  }
}
knn.roc=roc(datatest$label,knn.score)


# Random forest
train.X = datatrain[, c("NDAI", "SD", "CORR", "DF", "CF", "BF", "AF", "AN")]
test.X = datatest[, c("NDAI", "SD", "CORR", "DF", "CF", "BF", "AF", "AN")]
rf.fit = randomForest(x = train.X, y = as.factor(datatrain$label), mtry = 3)
rf.pred = predict(rf.fit, datatest)
rf.score=predict(rf.fit,datatest,type="prob")[,2]
rf.roc=roc(datatest$label,rf.score)
  
# Adaboost
adaboost.fit = gbm(as.character(label) ~ NDAI+SD+CORR+DF+CF+BF+AF+AN, 
            data = datatrain, 
            distribution = "adaboost", 
            n.trees = 3000, 
            interaction.depth = 4)
adaboost.pred = predict(adaboost.fit, datatest)
adaboost.pred[adaboost.pred >= 0.5] = 1
adaboost.pred[adaboost.pred < 0.5] = 0
adaboost.score=predict(adaboost.fit,datatest)
adaboost.roc=roc(datatest$label,adaboost.score)
```

```{r}
par(mfrow=c(2,4))
plot(glm.roc,print.thres=TRUE,print.auc=TRUE,main="Logistic Regression")
plot(lda.roc,print.thres=TRUE,print.auc=TRUE,main="LDA")
plot(qda.roc,print.thres=TRUE,print.auc=TRUE,main="QDA")
plot(nb.roc,print.thres=TRUE,print.auc=TRUE,main="Naive Bayes")
plot(knn.roc,print.thres=TRUE,print.auc=TRUE,main="knn")
plot(rf.roc,print.thres=TRUE,print.auc=TRUE,main="Random Forest")
plot(adaboost.roc,print.thres=TRUE,print.auc=TRUE,main="Adaboost")
```

```{r}
rocvalue=rbind(
  coords(glm.roc, "best"),
  coords(lda.roc, "best"),
  coords(qda.roc, "best"),
  coords(nb.roc, "best"),
  coords(knn.roc,"best"),
  coords(rf.roc, "best"),
  coords(adaboost.roc, "best")
)
rocvalue
```
ref:
https://stats.stackexchange.com/questions/61521/cut-off-point-in-a-roc-curve-is-there-a-simple-function
https://blog.csdn.net/solo7773/article/details/8699693?spm=1001.2101.3001.6650.11&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-11-8699693-blog-122552363.pc_relevant_aa&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-11-8699693-blog-122552363.pc_relevant_aa&utm_relevant_index=15

# c

```{r}
# logistic
glm.exp = rep(0, length(glm.probs))
glm.exp[glm.probs > rocvalue[1,1]] = 1
glm.cm=confusionMatrix(data=as.factor(glm.exp), reference = as.factor(datatest$label))
glm.cm
```


```{r}
# lda
lda.exp=rep(0,length(lda.score))
lda.exp[lda.score>rocvalue[2,1]]=1
lda.cm=confusionMatrix(data=as.factor(lda.exp), reference = as.factor(datatest$label))
lda.cm
```


```{r}
# qda
qda.exp=rep(0,length(qda.score))
qda.exp[qda.score>rocvalue[3,1]]=1
qda.cm=confusionMatrix(data=as.factor(qda.exp), reference = as.factor(datatest$label))
qda.cm
```


```{r}
# nb
nb.exp=rep(0,length(nb.score))
nb.exp[nb.score>rocvalue[4,1]]=1
nb.cm=confusionMatrix(data=as.factor(nb.exp), reference = as.factor(datatest$label))
nb.cm
```

```{r}
# knn
knn.exp=rep(0,length(knn.score))
knn.exp[knn.score>rocvalue[5,1]]=1
knn.cm=confusionMatrix(data=as.factor(knn.exp), reference = as.factor(datatest$label))
knn.cm
```


```{r}
# rf
rf.exp=rep(0,length(rf.score))
rf.exp[rf.score>rocvalue[6,1]]=1
rf.cm=confusionMatrix(data=as.factor(rf.exp), reference = as.factor(datatest$label))
rf.cm
```


```{r}
# adaboost
adaboost.exp=rep(0,length(adaboost.score))
adaboost.exp[adaboost.score>rocvalue[7,1]]=1
adaboost.cm=confusionMatrix(data=as.factor(adaboost.exp), reference = as.factor(datatest$label))
adaboost.cm
```

```{r}
sum=length(datatest$label)
round(glm.cm$table/sum,3)
round(lda.cm$table/sum,3)
round(qda.cm$table/sum,3)
round(nb.cm$table/sum,3)
round(knn.cm$table/sum,3)
round(rf.cm$table/sum,3)
round(adaboost.cm$table/sum,3)
```

```{r}
glm.cm$table
lda.cm$table
qda.cm$table
nb.cm$table
knn.cm$table
rf.cm$table
adaboost.cm$table
```


```{r}
library(pheatmap)
```

```{r}
pheatmap(glm.cm$table/sum,
         color = colorRampPalette(c("white","navy"))(50),
         cluster_rows=FALSE,cluster_cols=FALSE,
         display_numbers=TRUE,fontsize_number=30,fontsize_row=30,fontsize_col=30,
         width=5,height=5)
pheatmap(lda.cm$table/sum,
         color = colorRampPalette(c("white","navy"))(50),
         cluster_rows=FALSE,cluster_cols=FALSE,
         display_numbers=TRUE,fontsize_number=30,fontsize_row=30,fontsize_col=30,
         width=5,height=5)
pheatmap(qda.cm$table/sum,
         color = colorRampPalette(c("white","navy"))(50),
         cluster_rows=FALSE,cluster_cols=FALSE,
         display_numbers=TRUE,fontsize_number=30,fontsize_row=30,fontsize_col=30,
         width=5,height=5)
```


```{r}
train.X = scale(datatrain[, c("NDAI", "SD", "CORR", "DF", "CF", "BF", "AF", "AN")])
test.X = scale(datatest[, c("NDAI", "SD", "CORR", "DF", "CF", "BF", "AF", "AN")])

knn.pred=knn(train.X, test.X, datatrain$label, k=10)
table(knn.pred, datatest$label)
mean(knn.pred == datatest$label)


knn.pred=predict(caret::knn3(train.X, as.factor(datatrain$label), k=10), test.X)

pred = c()
for (i in 1:nrow(knn.pred)){
  prob0 = knn.pred[i,1]
  prob1 = knn.pred[i,2]
  pred=c(pred, ifelse(prob0>prob1, 0, 1))
}

knn.score=attr(knn.pred,"prob")

knn.roc=roc(datatest$label,knn.score)
plot(knn.roc,print.thres=TRUE,print.auc=TRUE,main="KNN")
knn.thre=coords(knn.roc,"best")[[1]]
knn.exp=rep(0,length(knn.score))
knn.exp[knn.score>=0.98]=1
knn.cm=confusionMatrix(data=as.factor(knn.exp), reference = as.factor(datatest$label))
knn.cm
```

# Task 4
## a Diagnostic plots
```{r}
# plot the tree: systematic
datatrain = image.syst.train %>% 
  filter(label != 0)
datatrain$label[datatrain$label == -1] = 0

library(rpart.plot)
rf.fit = rpart(as.factor(label)~NDAI+SD+CORR+DF+CF+BF+AF+AN, data=datatrain)
rpart.plot(rf.fit)

# plot the tree: buffer
datatrain = image.buf.train %>% 
  filter(label != 0)
datatrain$label[datatrain$label == -1] = 0

library(rpart.plot)
rf.fit = rpart(as.factor(label)~NDAI+SD+CORR+DF+CF+BF+AF+AN, data=datatrain)
rpart.plot(rf.fit)
ggsave("tree_buffer.png")
```









